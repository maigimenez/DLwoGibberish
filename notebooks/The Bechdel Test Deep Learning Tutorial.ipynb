{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actors, Awards, Country, Director, Error, Genre, Language, Metascore, Plot, Poster, Rated, Released, Response, Runtime, Title, Type, Writer, Year, imdbID, imdbRating, imdbVotes, imdb, year, title, budget, intgross, binary\n"
     ]
    }
   ],
   "source": [
    "BECHDEL_PATH = '../data/movies.csv'\n",
    "IMDB_PATH = '../data/imdb_data.json'\n",
    "\n",
    "movies_df = pd.read_csv(BECHDEL_PATH, delimiter=',')\n",
    "movies_df = movies_df[['imdb','year', 'title', 'budget', 'intgross', 'binary']]\n",
    "\n",
    "movies_df['binary'] = movies_df['binary'].str.replace('FAIL', '0')\n",
    "movies_df['binary'] = movies_df['binary'].str.replace('PASS', '1')\n",
    "movies_df['binary'] = movies_df['binary'].astype('int64')\n",
    "\n",
    "imdb_df = pd.read_json(IMDB_PATH)\n",
    "imdb_df = imdb_df[imdb_df['Type'] == 'movie']\n",
    "\n",
    "dataset_df =  imdb_df.join(movies_df, how='inner')\n",
    "print(\", \".join([k for k in dataset_df.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actors</th>\n",
       "      <th>Awards</th>\n",
       "      <th>Country</th>\n",
       "      <th>Director</th>\n",
       "      <th>Error</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Metascore</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Poster</th>\n",
       "      <th>...</th>\n",
       "      <th>Year</th>\n",
       "      <th>imdbID</th>\n",
       "      <th>imdbRating</th>\n",
       "      <th>imdbVotes</th>\n",
       "      <th>imdb</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>intgross</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jennifer Shirley, Blake Woodruff, Michael Rook...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>USA, Canada</td>\n",
       "      <td>Stewart Hendler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crime, Drama, Horror</td>\n",
       "      <td>English</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Sinister things begin happening to kidnappers ...</td>\n",
       "      <td>http://ia.media-imdb.com/images/M/MV5BMTM1Njgw...</td>\n",
       "      <td>...</td>\n",
       "      <td>2007</td>\n",
       "      <td>tt0435528</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6,584</td>\n",
       "      <td>tt1711425</td>\n",
       "      <td>2013</td>\n",
       "      <td>21 &amp;amp; Over</td>\n",
       "      <td>13000000</td>\n",
       "      <td>42195766.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michel Piccoli, Jerzy Stuhr, Renato Scarpa, Fr...</td>\n",
       "      <td>9 wins &amp; 12 nominations.</td>\n",
       "      <td>Italy, France</td>\n",
       "      <td>Nanni Moretti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comedy, Drama</td>\n",
       "      <td>Italian, German, Latin, English, Spanish, Poli...</td>\n",
       "      <td>64</td>\n",
       "      <td>A story centered on the relationship between t...</td>\n",
       "      <td>http://ia.media-imdb.com/images/M/MV5BMTQ4MjYz...</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>tt1456472</td>\n",
       "      <td>6.8</td>\n",
       "      <td>8,556</td>\n",
       "      <td>tt1343727</td>\n",
       "      <td>2012</td>\n",
       "      <td>Dredd 3D</td>\n",
       "      <td>45000000</td>\n",
       "      <td>40868994.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aml Ameen, Red Madrell, Noel Clarke, Adam Deacon</td>\n",
       "      <td>2 wins &amp; 1 nomination.</td>\n",
       "      <td>UK</td>\n",
       "      <td>Menhaj Huda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drama</td>\n",
       "      <td>English</td>\n",
       "      <td>N/A</td>\n",
       "      <td>A day in the life of a group of troubled 15-ye...</td>\n",
       "      <td>http://ia.media-imdb.com/images/M/MV5BMzg2Nzc2...</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>tt0435680</td>\n",
       "      <td>6.8</td>\n",
       "      <td>12,097</td>\n",
       "      <td>tt2024544</td>\n",
       "      <td>2013</td>\n",
       "      <td>12 Years a Slave</td>\n",
       "      <td>20000000</td>\n",
       "      <td>158607035.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paget Brewster, Jeff Branson, Jess Weixler, Ra...</td>\n",
       "      <td>4 wins &amp; 1 nomination.</td>\n",
       "      <td>USA</td>\n",
       "      <td>Ishai Setton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>English</td>\n",
       "      <td>N/A</td>\n",
       "      <td>A group of Connecticut locals enroll in an adu...</td>\n",
       "      <td>http://ia.media-imdb.com/images/M/MV5BMTg5OTQy...</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>tt0460721</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1,209</td>\n",
       "      <td>tt1272878</td>\n",
       "      <td>2013</td>\n",
       "      <td>2 Guns</td>\n",
       "      <td>61000000</td>\n",
       "      <td>132493015.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Keira Knightley, Viggo Mortensen, Michael Fass...</td>\n",
       "      <td>Nominated for 1 Golden Globe. Another 17 wins ...</td>\n",
       "      <td>UK, Germany, Canada, Switzerland</td>\n",
       "      <td>David Cronenberg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Biography, Drama, Thriller</td>\n",
       "      <td>English</td>\n",
       "      <td>76</td>\n",
       "      <td>A look at how the intense relationship between...</td>\n",
       "      <td>http://ia.media-imdb.com/images/M/MV5BMTU5Mjk3...</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>tt1571222</td>\n",
       "      <td>6.5</td>\n",
       "      <td>61,249</td>\n",
       "      <td>tt0453562</td>\n",
       "      <td>2013</td>\n",
       "      <td>42</td>\n",
       "      <td>40000000</td>\n",
       "      <td>95020213.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Actors  \\\n",
       "0  Jennifer Shirley, Blake Woodruff, Michael Rook...   \n",
       "1  Michel Piccoli, Jerzy Stuhr, Renato Scarpa, Fr...   \n",
       "2   Aml Ameen, Red Madrell, Noel Clarke, Adam Deacon   \n",
       "3  Paget Brewster, Jeff Branson, Jess Weixler, Ra...   \n",
       "4  Keira Knightley, Viggo Mortensen, Michael Fass...   \n",
       "\n",
       "                                              Awards  \\\n",
       "0                                                N/A   \n",
       "1                           9 wins & 12 nominations.   \n",
       "2                             2 wins & 1 nomination.   \n",
       "3                             4 wins & 1 nomination.   \n",
       "4  Nominated for 1 Golden Globe. Another 17 wins ...   \n",
       "\n",
       "                            Country          Director Error  \\\n",
       "0                       USA, Canada   Stewart Hendler   NaN   \n",
       "1                     Italy, France     Nanni Moretti   NaN   \n",
       "2                                UK       Menhaj Huda   NaN   \n",
       "3                               USA      Ishai Setton   NaN   \n",
       "4  UK, Germany, Canada, Switzerland  David Cronenberg   NaN   \n",
       "\n",
       "                        Genre  \\\n",
       "0        Crime, Drama, Horror   \n",
       "1               Comedy, Drama   \n",
       "2                       Drama   \n",
       "3                      Comedy   \n",
       "4  Biography, Drama, Thriller   \n",
       "\n",
       "                                            Language Metascore  \\\n",
       "0                                            English       N/A   \n",
       "1  Italian, German, Latin, English, Spanish, Poli...        64   \n",
       "2                                            English       N/A   \n",
       "3                                            English       N/A   \n",
       "4                                            English        76   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  Sinister things begin happening to kidnappers ...   \n",
       "1  A story centered on the relationship between t...   \n",
       "2  A day in the life of a group of troubled 15-ye...   \n",
       "3  A group of Connecticut locals enroll in an adu...   \n",
       "4  A look at how the intense relationship between...   \n",
       "\n",
       "                                              Poster  ...    Year     imdbID  \\\n",
       "0  http://ia.media-imdb.com/images/M/MV5BMTM1Njgw...  ...    2007  tt0435528   \n",
       "1  http://ia.media-imdb.com/images/M/MV5BMTQ4MjYz...  ...    2011  tt1456472   \n",
       "2  http://ia.media-imdb.com/images/M/MV5BMzg2Nzc2...  ...    2006  tt0435680   \n",
       "3  http://ia.media-imdb.com/images/M/MV5BMTg5OTQy...  ...    2006  tt0460721   \n",
       "4  http://ia.media-imdb.com/images/M/MV5BMTU5Mjk3...  ...    2011  tt1571222   \n",
       "\n",
       "  imdbRating imdbVotes       imdb  year             title    budget  \\\n",
       "0        5.8     6,584  tt1711425  2013     21 &amp; Over  13000000   \n",
       "1        6.8     8,556  tt1343727  2012          Dredd 3D  45000000   \n",
       "2        6.8    12,097  tt2024544  2013  12 Years a Slave  20000000   \n",
       "3        6.5     1,209  tt1272878  2013            2 Guns  61000000   \n",
       "4        6.5    61,249  tt0453562  2013                42  40000000   \n",
       "\n",
       "      intgross binary  \n",
       "0   42195766.0      0  \n",
       "1   40868994.0      1  \n",
       "2  158607035.0      0  \n",
       "3  132493015.0      0  \n",
       "4   95020213.0      0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = dataset_df.head(1500)\n",
    "X_train = training['Plot'].tolist()\n",
    "y_train = training['binary'].tolist()\n",
    "\n",
    "testing = dataset_df.tail(284)\n",
    "X_test = testing['Plot'].tolist()\n",
    "\n",
    "y_test = testing['binary'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the data\n",
    "Each sentece has different lenghts. But our CNN models needs a fixed-size input. Hence, we include zeros at the ende so each sentence will have the same number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_words(sentences):\n",
    "    \"\"\" Return the maximum number of words in the dataset \"\"\"\n",
    "    max_num_words = -1\n",
    "    for sentence in sentences:\n",
    "        len_sentence = len(sentence)\n",
    "        if len_sentence > max_num_words:\n",
    "            max_num_words =len_sentence\n",
    "    return max_num_words\n",
    "\n",
    "def pad(sentences, max_sequence=None):\n",
    "    \"\"\" Pad all the sententences in order to have sequences of same length \"\"\"\n",
    "    if not max_sequence:\n",
    "        max_sequence = max_words(sentences)\n",
    "        \n",
    "    padded_dataset = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        len_padding = max_sequence - len(sentence)\n",
    "        padded_sentence = sentence + ['</pad>'] * len_padding\n",
    "        assert len(padded_sentence) == max_sequence\n",
    "        padded_dataset.append(padded_sentence)\n",
    "    \n",
    "    assert len(padded_dataset) == len(sentences)\n",
    "    return padded_dataset, max_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad, max_sequence = pad(X_train)\n",
    "test_pad, _ = pad(X_test, max_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sinister things begin happening to kidnappers who are holding a young boy for ransom in a remote cabin.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sinister things begin happening to kidnappers who are holding a young boy for ransom in a remote cabin. </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad> </pad>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the lookup matrix and the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "def build_vocab(dataset, to_lower=False):\n",
    "    \"\"\" Create a lookup table and a list with the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): a matrix with the words from the dataset\n",
    "\n",
    "    Returns:\n",
    "        vocab_sorted (list): list of the words sorted by its frequency\n",
    "        lookup (dict): a dictionary with the lookup table. The keys are\n",
    "            the words and the values are the indexes.\n",
    "    \"\"\"\n",
    "    # Count how many times a word appear in the dataset\n",
    "    word_counts = Counter(chain(*dataset))\n",
    "    # Create a list with the most common words sorted.\n",
    "    # The position will be the index of the lookup table.\n",
    "    vocab_sorted = []\n",
    "    for word, _ in word_counts.most_common():\n",
    "        if to_lower:\n",
    "            word = word.lower()\n",
    "        if word not in vocab_sorted:\n",
    "            vocab_sorted.append(word)\n",
    "    vocab_sorted.append('<oov>')\n",
    "    # Create a lookup table using a dictionary. Map each index with a word\n",
    "    lookup = {word: index for index, word in enumerate(vocab_sorted)}\n",
    "\n",
    "    # TODO: Move this to the unittest\n",
    "    assert len(list(lookup.keys())) == len(vocab_sorted)\n",
    "\n",
    "    return vocab_sorted, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted, lookup = build_vocab(train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 10060\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_sorted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_indexes(sentences, vocabulary):\n",
    "    \"\"\" Convert a list of sentences to its corresponding indices.\n",
    "\n",
    "    Args:\n",
    "        sentences: a list where each element is a lists of words (sentences)\n",
    "        vocabulary: a dictionary of words and its corresponding index\n",
    "\n",
    "    Returns:\n",
    "        a list where each element is a list of indexes (sentences)\n",
    "\n",
    "    \"\"\"\n",
    "    senteces_idx = []\n",
    "    for sentence in sentences:\n",
    "        aux_idx = []\n",
    "        for word in sentence:\n",
    "            # If the word is in the vocabulary get its index otherwise use the <oov> index\n",
    "            if word in vocabulary:\n",
    "                word_idx = vocabulary[word]\n",
    "            else:\n",
    "                word_idx = vocabulary['<oov>']\n",
    "            aux_idx.append(word_idx)\n",
    "        senteces_idx.append(aux_idx)\n",
    "    return np.array(senteces_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = to_indexes(train_pad, lookup)\n",
    "test_idx = to_indexes(test_pad, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3209,  311,  635, 1793,    3, 3210,   16,   23,  958,    1,   25,\n",
       "        100,   12, 3211,    6,    1,  312, 3212,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "sequence_length = max_sequence\n",
    "embedding_dim = 50\n",
    "num_filters = 100\n",
    "dropout_prob = (0.5,)\n",
    "hidden_dims = (1024, 128)\n",
    "filters_h = (3, 4, 5)\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "\n",
    "output_classes =  1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_rand(sequence_length, embedding_dim, num_filters, filtes_h, \n",
    "              dropout_prob, hidden_dims, output_neurons, verbose=False):\n",
    "    # Input\n",
    "    embedding_weights = None\n",
    "    input_shape = (sequence_length,)\n",
    "    model_input = Input(shape=input_shape)\n",
    "    embedding_input = Embedding(len(vocab_sorted), \n",
    "                                embedding_dim, \n",
    "                                input_length=sequence_length, \n",
    "                                weights=embedding_weights, name=\"embedding\")(model_input)\n",
    "\n",
    "    print(\"Embedding dim:\", len(vocab_sorted), embedding_dim)\n",
    "        \n",
    "    # CNNs\n",
    "    convs = []\n",
    "    for fh in filtes_h:\n",
    "        conv = Convolution1D(filters=num_filters,\n",
    "                             kernel_size=fh,\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(embedding_input)\n",
    "        pool = MaxPooling1D(pool_size=2)(conv)\n",
    "        flatten = Flatten()(pool)\n",
    "        convs.append(flatten)\n",
    "\n",
    "    concat_layer = Concatenate(axis=-1)(convs)\n",
    "    dropout_cnn = Dropout(dropout_prob[0])(concat_layer)\n",
    "\n",
    "    h1 = Dense(hidden_dims[0], activation=\"relu\")(dropout_cnn)\n",
    "    h2 = Dense(hidden_dims[1], activation=\"relu\")(h1)\n",
    "    model_output = Dense(output_neurons, activation=\"sigmoid\")(h2)\n",
    "\n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 10060 50\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 386)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 386, 50)       503000      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 384, 100)      15100       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 383, 100)      20100       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 382, 100)      25100       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 192, 100)      0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 191, 100)      0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 191, 100)      0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 19200)         0           max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 19100)         0           max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 19100)         0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 57400)         0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 57400)         0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          58778624    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           131200      dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             129         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 59,473,253\n",
      "Trainable params: 59,473,253\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "model = cnn_model_rand(sequence_length, embedding_dim, num_filters, filters_h, \n",
    "                         dropout_prob, hidden_dims, output_classes, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 56s - loss: 0.6977 - acc: 0.5113    \n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 49s - loss: 0.6926 - acc: 0.5313    \n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 48s - loss: 0.6912 - acc: 0.5387    \n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 48s - loss: 0.6881 - acc: 0.5333    \n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 49s - loss: 0.6870 - acc: 0.5513    \n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 49s - loss: 0.6844 - acc: 0.5740    \n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 49s - loss: 0.6780 - acc: 0.5793    \n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 48s - loss: 0.6695 - acc: 0.6247    \n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 48s - loss: 0.6585 - acc: 0.6593    \n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 47s - loss: 0.6380 - acc: 0.7460    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1261e6278>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_idx, np.array(y_train), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.626760560862\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_idx, np.array(y_test), verbose=0)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
